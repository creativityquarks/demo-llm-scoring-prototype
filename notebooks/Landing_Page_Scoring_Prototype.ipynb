{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d3fc93e",
      "metadata": {
        "id": "2d3fc93e"
      },
      "source": [
        "# Landing Page Scoring Prototype (FastAPI + OpenAI)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3949cde6",
      "metadata": {
        "id": "3949cde6"
      },
      "source": [
        "This notebook launches a **FastAPI** server that scores landing pages using an **LLM**.\n",
        "\n",
        "- `POST /score`: analyze one page (clarity, credibility, CTA)\n",
        "- `POST /compare`: compare **before** vs **after**\n",
        "\n",
        "We defined the main function, `llm_score_page`, in such a way it will auto-fallback to mock when needed, i.e.\n",
        "- for the purpose of the demo, to avoid having to provide a key, from a paid API ;\n",
        "- if the LLLM API is failing to answer the requests\n",
        "\n",
        "The mock, `_mock_score`, has been designed very simple and heuristics so the demo yields sensible, deterministic output.\n",
        "\n",
        "Parameters: in the section **2) Set your OpenAI API key** of this notebook, if you have an Openai API key, you can set the environment variable `OPENAI_API_KEY` and `USE_MOCK` to `\"0\"`.\n",
        "Otherwise you can just set the environment variable `USE_MOCK` to `\"1\"`.\n",
        "\n",
        "other wise just leave it .\n",
        "\n",
        "> Tip: Run cells top-to-bottom. The server starts in the background so you can test directly from the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15902709",
      "metadata": {
        "id": "15902709"
      },
      "source": [
        "## 1) Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c5e9abd",
      "metadata": {
        "id": "3c5e9abd"
      },
      "outputs": [],
      "source": [
        "# If running locally, uncomment to install\n",
        "!pip install -q fastapi uvicorn pydantic openai>=1.0.0 nest_asyncio requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ac2aaa",
      "metadata": {
        "id": "09ac2aaa"
      },
      "source": [
        "## 2) Set your OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892ac996",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "892ac996",
        "outputId": "ed3979f5-eda7-4e90-9595-54a3fc5fccd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USE_MOCK environment variable's value is 0\n",
            "⚠️ Mock mode is OFF, the demo uses the API key provided.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"USE_MOCK\"] = \"0\"\n",
        "\n",
        "print(\"USE_MOCK environment variable's value is\", os.environ.get(\"USE_MOCK\"))\n",
        "\n",
        "if os.getenv(\"USE_MOCK\") == \"0\" :\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Uncomment and paste your key\n",
        "\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\") :\n",
        "          print(\"⚠️ Mock mode is OFF, OPENAI_API_KEY is not set. Set it via environment or in this cell before starting the server.\")\n",
        "    else :\n",
        "          print(\"⚠️ Mock mode is OFF, the demo uses the API key provided.\")\n",
        "\n",
        "else :\n",
        "      print(\"Mock mode is ON, if you don't have an API key we propose a simple and heuristic scoring\")\n",
        "\n",
        "\n",
        "os.environ[\"PORT\"] = \"8000\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74dcaa89",
      "metadata": {
        "id": "74dcaa89"
      },
      "source": [
        "## 3) Define FastAPI app and LLM helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36da1d92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36da1d92",
        "outputId": "ed58618c-4f1d-4687-d74f-3a2441784726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:128: SyntaxWarning: invalid escape sequence '\\{'\n",
            "<>:128: SyntaxWarning: invalid escape sequence '\\{'\n",
            "/tmp/ipython-input-1562034320.py:128: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  m = re.search(r\"\\{[\\s\\S]*\\}$\", raw_text.strip())\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from fastapi import FastAPI\n",
        "from openai import OpenAI\n",
        "import json, re\n",
        "import requests, json\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "def _mock_score(html: str, criteria: List[str]) -> Dict[str, Any]:\n",
        "    # Very simple heuristics so the demo yields sensible, deterministic output\n",
        "    txt = re.sub(r\"<[^>]+>\", \" \", html or \"\").lower()\n",
        "    have_h1   = bool(re.search(r\"<h1[^>]*>.*?</h1>\", html or \"\", flags=re.I|re.S))\n",
        "    have_title= bool(re.search(r\"<title[^>]*>.*?</title>\", html or \"\", flags=re.I|re.S))\n",
        "    have_btn  = (\"<button\" in (html or \"\").lower()) or any(k in txt for k in [\"get started\",\"start free\",\"sign up\",\"contact\",\"try free\"])\n",
        "    trust_kw  = sum(k in txt for k in [\"trusted by\",\"testimonials\",\"review\",\"reviews\",\"privacy\",\"secure\",\"https\",\"iso\",\"gdpr\",\"clients\",\"partners\"])\n",
        "    words     = len(txt.split())\n",
        "\n",
        "    scores: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    if \"clarity\" in criteria:\n",
        "        s = 4 + (3 if have_h1 else 0) + (2 if have_title else 0) + (1 if 50 <= words <= 400 else 0)\n",
        "        s = max(1, min(10, s))\n",
        "        scores[\"clarity\"] = {\"score\": s, \"feedback\": (\"Clear headline.\" if have_h1 else \"Add a clear H1.\") + (\" Keep copy concise.\" if words>400 else \"\")}\n",
        "\n",
        "    if \"credibility\" in criteria:\n",
        "        s = 3 + min(7, trust_kw)  # up to +7 from trust signals\n",
        "        s = max(1, min(10, s))\n",
        "        fb_parts = []\n",
        "        if trust_kw == 0: fb_parts.append(\"Add testimonials/trust badges.\")\n",
        "        if \"https\" not in txt: fb_parts.append(\"Show security/privacy info.\")\n",
        "        scores[\"credibility\"] = {\"score\": s, \"feedback\": \" \".join(fb_parts) or \"Good trust signals.\"}\n",
        "\n",
        "    if \"cta\" in criteria:\n",
        "        s = 4 + (4 if have_btn else 0) + (2 if have_h1 else 0)\n",
        "        s = max(1, min(10, s))\n",
        "        scores[\"cta\"] = {\"score\": s, \"feedback\": (\"CTA visible.\" if have_btn else \"Add a prominent CTA.\")}\n",
        "\n",
        "    # Fill missing criteria if any\n",
        "    for c in criteria:\n",
        "        scores.setdefault(c, {\"score\": 5, \"feedback\": \"OK\"})\n",
        "\n",
        "    overall = round(sum(v[\"score\"] for v in scores.values())/len(scores), 2)\n",
        "    return {\"scores\": scores, \"overall\": overall, \"notes\": \"Mock mode: heuristic scoring\"}\n",
        "\n",
        "# llm_score_page defined to auto-fallback to mock when needed\n",
        "def llm_score_page(html: str, url: Optional[str], criteria: Optional[List[str]] = None, model: str = \"gpt-5-mini\") -> Dict[str, Any]:\n",
        "    from openai import OpenAI\n",
        "    from openai import RateLimitError, AuthenticationError, APIStatusError  # available in new SDKs; if missing, we’ll catch generic Exception\n",
        "    client = OpenAI()\n",
        "\n",
        "    criteria = criteria or [\"clarity\",\"credibility\",\"cta\"]\n",
        "\n",
        "    # Force mock if requested or no key present\n",
        "    if USE_MOCK or not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        return _mock_score(html, criteria)\n",
        "\n",
        "    # Build messages\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"You are an assistant that evaluates marketing landing pages.\\n\"\n",
        "        \"Score each requested criterion from 1 to 10 and give concise, actionable feedback (max ~25 words each).\\n\"\n",
        "        \"Be consistent and fair across pages. When content is missing, explain briefly.\\n\"\n",
        "        \"Return ONLY valid JSON. Use integers for scores.\"\n",
        "    )\n",
        "    crit_list = \"\\n\".join(f\"- {c}\" for c in criteria)\n",
        "    user_prompt = f\"\"\"Evaluate the landing page below.\n",
        "URL: {url or \"N/A\"}\n",
        "\n",
        "Criteria:\n",
        "{crit_list}\n",
        "\n",
        "Return a JSON object with:\n",
        "{{\n",
        "  \"scores\": {{\n",
        "    \"<criterion>\": {{ \"score\": <int 1-10>, \"feedback\": \"<string>\" }}\n",
        "  }},\n",
        "  \"overall\": <float>,\n",
        "  \"notes\": \"<short rationale>\"\n",
        "}}\n",
        "\n",
        "Page HTML (truncated OK):\n",
        "<<<HTML_START>>>\n",
        "{html}\n",
        "<<<HTML_END>>>\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    # Try OpenAI; if quota/auth fails, fall back to mock (but still return 200 JSON)\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model, temperature=0.2, messages=messages,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        content = resp.choices[0].message.content\n",
        "        data = json.loads(content)\n",
        "        # Fill any missing parts\n",
        "        data.setdefault(\"scores\", {})\n",
        "        for c in criteria:\n",
        "            data[\"scores\"].setdefault(c, {\"score\": 0, \"feedback\": \"\"})\n",
        "        if \"overall\" not in data or not isinstance(data[\"overall\"], (int,float)):\n",
        "            vals = [v[\"score\"] for v in data[\"scores\"].values()]\n",
        "            data[\"overall\"] = round(sum(vals)/len(vals), 2) if vals else 0.0\n",
        "        data.setdefault(\"notes\", \"\")\n",
        "        return data\n",
        "\n",
        "    except (RateLimitError, AuthenticationError, APIStatusError) as e:\n",
        "        # Quota/auth/server issue → degrade gracefully\n",
        "        mock = _mock_score(html, criteria)\n",
        "        mock[\"notes\"] = f\"Mock fallback due to LLM error: {type(e).__name__}\"\n",
        "        return mock\n",
        "\n",
        "    except Exception as e:\n",
        "        # Any other unexpected error → degrade gracefully\n",
        "        mock = _mock_score(html, criteria)\n",
        "        mock[\"notes\"] = f\"Mock fallback due to error: {e.__class__.__name__}\"\n",
        "        return mock\n",
        "\n",
        "'''\n",
        "def normalize_output(raw_text: str, criteria: List[str]) -> Dict[str, Any]:\n",
        "    try:\n",
        "        data = json.loads(raw_text)\n",
        "    except Exception:\n",
        "        import re\n",
        "        m = re.search(r\"\\{[\\s\\S]*\\}$\", raw_text.strip())\n",
        "        if not m:\n",
        "            raise\n",
        "        data = json.loads(m.group(0))\n",
        "\n",
        "    data.setdefault(\"scores\", {})\n",
        "    for c in criteria:\n",
        "        data[\"scores\"].setdefault(c, {\"score\": 0, \"feedback\": \"\"})\n",
        "\n",
        "    vals = [v.get(\"score\") for v in data[\"scores\"].values() if isinstance(v.get(\"score\"), (int, float))]\n",
        "    data[\"overall\"] = round(sum(vals)/len(vals), 2) if vals else 0.0\n",
        "    data.setdefault(\"notes\", \"\")\n",
        "    return data\n",
        "\n",
        "def llm_score_page(html: str, url: Optional[str], criteria: Optional[List[str]] = None, model: str = \"gpt-5-mini\") -> Dict[str, Any]:\n",
        "    criteria = criteria or DEFAULT_CRITERIA\n",
        "    messages = build_messages(html, url, criteria)\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=0.2,\n",
        "            messages=messages,\n",
        "            response_format={ \"type\": \"json_object\" }\n",
        "        )\n",
        "        content = resp.choices[0].message.content\n",
        "    except Exception:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=model,\n",
        "            temperature=0.2,\n",
        "            messages=messages\n",
        "        )\n",
        "        content = resp.choices[0].message.content\n",
        "\n",
        "    return normalize_output(content, criteria)\n",
        "\n",
        "'''\n",
        "\n",
        "class ScoreRequest(BaseModel):\n",
        "    html: str = Field(..., description=\"Raw HTML of the page\")\n",
        "    url: Optional[str] = Field(None, description=\"Optional URL (for context)\")\n",
        "    criteria: Optional[List[str]] = Field(None, description=\"List of criteria to evaluate\")\n",
        "\n",
        "class ScoreResponse(BaseModel):\n",
        "    scores: Dict[str, Dict[str, Any]]\n",
        "    overall: float\n",
        "    notes: str\n",
        "\n",
        "class CompareRequest(BaseModel):\n",
        "    before_html: str\n",
        "    after_html: str\n",
        "    url: Optional[str] = None\n",
        "    criteria: Optional[List[str]] = None\n",
        "\n",
        "class CompareResponse(BaseModel):\n",
        "    before: ScoreResponse\n",
        "    after: ScoreResponse\n",
        "    delta: Dict[str, float]\n",
        "\n",
        "app = FastAPI(title=\"Landing Page Scoring Prototype - API\", version=\"0.1.0\")\n",
        "\n",
        "@app.get(\"/healthz\")\n",
        "def healthz():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "@app.post(\"/score\", response_model=ScoreResponse)\n",
        "def score_page(req: ScoreRequest):\n",
        "    result = llm_score_page(req.html, req.url, req.criteria)\n",
        "    return result\n",
        "\n",
        "@app.post(\"/compare\", response_model=CompareResponse)\n",
        "def compare_pages(req: CompareRequest):\n",
        "    criteria = req.criteria or [\"clarity\", \"credibility\", \"cta\"]\n",
        "    before = llm_score_page(req.before_html, req.url, criteria)\n",
        "    after = llm_score_page(req.after_html, req.url, criteria)\n",
        "    delta = {}\n",
        "    for c in criteria + [\"overall\"]:\n",
        "        b = before[\"scores\"][c][\"score\"] if c in before[\"scores\"] else before[\"overall\"]\n",
        "        a = after[\"scores\"][c][\"score\"] if c in after[\"scores\"] else after[\"overall\"]\n",
        "        delta[c] = round((a - b), 2)\n",
        "    return {\"before\": before, \"after\": after, \"delta\": delta}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94bc766b",
      "metadata": {
        "id": "94bc766b"
      },
      "source": [
        "## 4) Start FastAPI (background)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bde88547",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde88547",
        "outputId": "72680cdb-d42e-42b6-a991-313fc19eee5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 API running at http://127.0.0.1:8002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [1164]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio, threading, uvicorn, socket\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def find_free_port(start=8000, end=8100):\n",
        "    import socket\n",
        "    for port in range(start, end+1):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            try:\n",
        "                s.bind((\"127.0.0.1\", port))\n",
        "                return port\n",
        "            except OSError:\n",
        "                continue\n",
        "    raise RuntimeError(\"No free port found in range.\")\n",
        "\n",
        "PORT = find_free_port()\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=PORT, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "print(f\"🚀 API running at http://127.0.0.1:{PORT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ed1f27",
      "metadata": {
        "id": "81ed1f27"
      },
      "source": [
        "## 5) Test call to `/score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2283a225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2283a225",
        "outputId": "e5638f0d-8acb-45f5-ffcc-5ab78457a889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     127.0.0.1:49634 - \"POST /score HTTP/1.1\" 200 OK\n",
            "200 application/json\n",
            "{\n",
            "  \"scores\": {\n",
            "    \"clarity\": {\n",
            "      \"score\": 9,\n",
            "      \"feedback\": \"Clear headline.\"\n",
            "    },\n",
            "    \"credibility\": {\n",
            "      \"score\": 4,\n",
            "      \"feedback\": \"Show security/privacy info.\"\n",
            "    },\n",
            "    \"cta\": {\n",
            "      \"score\": 10,\n",
            "      \"feedback\": \"CTA visible.\"\n",
            "    }\n",
            "  },\n",
            "  \"overall\": 7.67,\n",
            "  \"notes\": \"Mock fallback due to LLM error: RateLimitError\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import requests, json\n",
        "\n",
        "sample_html = \"\"\"\n",
        "<html>\n",
        "  <head><title>Grow your leads fast</title></head>\n",
        "  <body>\n",
        "    <h1>Boost your lead generation</h1>\n",
        "    <p>Join 1,200+ businesses using our platform.</p>\n",
        "    <button>Get started</button>\n",
        "    <footer>No testimonials yet.</footer>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# Better handling errors - Make the client test more robust (avoid JSONDecodeError)\n",
        "resp = requests.post(f\"http://127.0.0.1:{PORT}/score\", json={\n",
        "    \"html\": sample_html,\n",
        "    \"url\": \"https://example.com\",\n",
        "    \"criteria\": [\"clarity\", \"credibility\", \"cta\"]\n",
        "})\n",
        "\n",
        "print(resp.status_code, resp.headers.get(\"content-type\"))\n",
        "if \"application/json\" in (resp.headers.get(\"content-type\") or \"\"):\n",
        "    print(json.dumps(resp.json(), indent=2))\n",
        "else:\n",
        "    print(resp.text[:1000])  # show text body if not JSON\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8140f48",
      "metadata": {
        "id": "b8140f48"
      },
      "source": [
        "## 6) Test call to `/compare`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35eda3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f35eda3b",
        "outputId": "06f31178-d584-4ab1-b0f9-9563810c979a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     127.0.0.1:33656 - \"POST /compare HTTP/1.1\" 200 OK\n",
            "200\n",
            "{\n",
            "  \"before\": {\n",
            "    \"scores\": {\n",
            "      \"clarity\": {\n",
            "        \"score\": 7,\n",
            "        \"feedback\": \"Clear headline.\"\n",
            "      },\n",
            "      \"credibility\": {\n",
            "        \"score\": 3,\n",
            "        \"feedback\": \"Add testimonials/trust badges. Show security/privacy info.\"\n",
            "      },\n",
            "      \"cta\": {\n",
            "        \"score\": 10,\n",
            "        \"feedback\": \"CTA visible.\"\n",
            "      }\n",
            "    },\n",
            "    \"overall\": 6.67,\n",
            "    \"notes\": \"Mock fallback due to LLM error: RateLimitError\"\n",
            "  },\n",
            "  \"after\": {\n",
            "    \"scores\": {\n",
            "      \"clarity\": {\n",
            "        \"score\": 7,\n",
            "        \"feedback\": \"Clear headline.\"\n",
            "      },\n",
            "      \"credibility\": {\n",
            "        \"score\": 4,\n",
            "        \"feedback\": \"Show security/privacy info.\"\n",
            "      },\n",
            "      \"cta\": {\n",
            "        \"score\": 10,\n",
            "        \"feedback\": \"CTA visible.\"\n",
            "      }\n",
            "    },\n",
            "    \"overall\": 7.0,\n",
            "    \"notes\": \"Mock fallback due to LLM error: RateLimitError\"\n",
            "  },\n",
            "  \"delta\": {\n",
            "    \"clarity\": 0.0,\n",
            "    \"credibility\": 1.0,\n",
            "    \"cta\": 0.0,\n",
            "    \"overall\": 0.33\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import requests, json\n",
        "before_html = \"\"\"\n",
        "<html>\n",
        "  <body>\n",
        "    <h1>Boost your lead generation</h1>\n",
        "    <p>We help teams grow.</p>\n",
        "    <button>Learn more</button>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "after_html = \"\"\"\n",
        "<html>\n",
        "  <body>\n",
        "    <h1>Boost your lead generation</h1>\n",
        "    <p>Join 1,200+ businesses using our platform. 30-day free trial.</p>\n",
        "    <button>Start free trial</button>\n",
        "    <section><h2>Trusted by</h2><ul><li>ACME</li><li>Globex</li></ul></section>\n",
        "  </body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "resp = requests.post(f\"http://127.0.0.1:{PORT}/compare\", json={\n",
        "    \"before_html\": before_html,\n",
        "    \"after_html\": after_html,\n",
        "    \"url\": \"https://example.com\",\n",
        "    \"criteria\": [\"clarity\", \"credibility\", \"cta\"]\n",
        "})\n",
        "print(resp.status_code)\n",
        "print(json.dumps(resp.json(), indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
